{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b218da21",
   "metadata": {},
   "source": [
    "# Optimizers in Deep Learning\n",
    "\n",
    "Optimizers are algorithms used to minimize the loss function by updating the model's weights during training. The goal is to find the best set of weights that allow the model to make accurate predictions. Below are some commonly used optimizers:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Gradient Descent (GD)\n",
    "- **Description:** The most basic optimization method. It updates weights by moving in the direction of the negative gradient of the loss function.\n",
    "- **Types:**\n",
    "  - **Batch Gradient Descent:** Uses the entire dataset to calculate gradients.\n",
    "  - **Stochastic Gradient Descent (SGD):** Uses one sample at a time.\n",
    "  - **Mini-Batch Gradient Descent:** Uses small batches of data.\n",
    "- **Drawback:** Can be slow and may get stuck in local minima.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Stochastic Gradient Descent (SGD)\n",
    "- **Description:** Updates weights using a single sample at a time.\n",
    "- **Advantage:** Faster for large datasets but can lead to noisy updates, causing slower convergence.\n",
    "  \n",
    "---\n",
    "\n",
    "## 3. Momentum\n",
    "- **Description:** Improves SGD by adding a fraction of the previous update to the current update. This helps in accelerating convergence and reducing oscillations.\n",
    "- **Update:** \n",
    "  \n",
    "- **Advantage:** Helps escape local minima and speeds up convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. RMSProp (Root Mean Square Propagation)\n",
    "- **Description:** Adjusts the learning rate for each weight by dividing it by the square root of the average squared gradients.\n",
    "- **Advantage:** Works well for non-stationary problems and helps in maintaining a stable learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Adam (Adaptive Moment Estimation)\n",
    "- **Description:** Combines **Momentum** and **RMSProp**, tracking both the average of past gradients (Momentum) and the average of squared gradients (RMSProp).\n",
    "- **Advantage:** Works well for large datasets and has fast convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. AdaGrad (Adaptive Gradient Algorithm)\n",
    "- **Description:** Adjusts the learning rate for each weight based on how frequently that weight has been updated. \n",
    "- **Drawback:** Can lead to very small learning rates after several updates.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. AdaDelta\n",
    "- **Description:** An improvement on AdaGrad that limits the accumulation of past gradients, which prevents the learning rate from getting too small.\n",
    "  \n",
    "---\n",
    "\n",
    "## 8. Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
    "- **Description:** An extension of Adam that includes **Nesterov momentum**, allowing for more informed weight updates by looking ahead.\n",
    "\n",
    "---\n",
    "\n",
    "This list of optimizers highlights the key methods used in deep learning and their respective advantages and drawbacks. Each optimizer has its use cases depending on the problem being solved.\n",
    "\n",
    "Let me know if you would like to dive deeper into any specific optimizer!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
